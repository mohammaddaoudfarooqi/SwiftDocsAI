[[["### File: /Users/mdf/Code/SwiftDocsAI/requirements.txt\n### Chunk: 1\n", "### File: /Users/mdf/Code/SwiftDocsAI/sample.env\n### Chunk: 1\n", "### File: /Users/mdf/Code/SwiftDocsAI/main.py\n### Chunk: 1\n"], ["boto3==1.35.90\npython-dotenv==1.0.1", "AWS_ACCESS_KEY_ID=\"\"\nAWS_SECRET_ACCESS_KEY=\"\"", "import os\nimport json\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor\nimport boto3\nfrom botocore.config import Config\nfrom dotenv import load_dotenv\nimport random\nimport time\n\n# Directory path\nlogs_dir = \"logs\"\n\n# Check if the directory exists\nif not os.path.exists(logs_dir):\n    # Create the directory\n    os.makedirs(logs_dir)\n    print(f\"Created directory: {logs_dir}\")\nelse:\n    print(f\"Directory '{logs_dir}' already exists.\")\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"logs/process.log\"),\n        logging.StreamHandler(),  # logging.infos logs to the console\n    ],\n)\n\n# Log start of script execution\nlogging.info(\"Script execution started.\")\n\n# Load environment variables from .env file\nload_dotenv()\nlogging.info(\"Environment variables loaded.\")\n\n# Constants for processing with Claude\nDEFAULT_MAX_CHARS = 400000\nDEFAULT_MAX_WORDS = 90000\n\nCHUNK_LIMIT = 3  # Configurable limit to decide sequential or parallel processing\n\n# Instruction prompt that will be appended to each chunk\nINSTRUCTION_PROMPT = \"\"\"\n**Instructions for Generating Comprehensive Technical Documentation**\n\n**Objective:** Create a complete and detailed technical documentation for the provided application. The document should serve as a standalone guide for all users, from beginners to experienced developers, ensuring clarity, organization, and thoroughness throughout.  \n\n**Guidelines for Structure and Content:**  \n\n1. **General Requirements:**  \n   - **Standalone Sections:** Each section must be self-contained, including all necessary context to make sense independently. Avoid referencing other sections with phrases like \"refer to the section above\" or \"as mentioned earlier.\"  \n   - **Integration of Context:** Use all available information from the provided context to create exhaustive descriptions for each section.  \n   - **Detailed Coverage:** Ensure no detail is omitted. The document should flow logically and provide comprehensive coverage of the application's functionality, architecture, usage, and maintenance.  \n   - For every section, integrate all relevant details from the current and previous context to create a standalone and exhaustive description. \n\n2. **Document Formatting:**  \n   - Use **Markdown** syntax for clear formatting.  \n   - Include headings, subheadings, bullet points, code blocks, and hyperlinks where appropriate.  \n   - Provide code snippets, configuration examples, diagrams, or tables to illustrate concepts.  \n\n3. **Required Sections:**  \n   The documentation should follow the structure outlined below:  \n\n   **Title:**  \n   `[Project Name] Documentation`  \n\n   **Table of Contents:**  \n   List all major sections and subsections with clickable links if applicable.  \n\n   ### 1. Overview  \n   - Provide a concise introduction to the project, its purpose, and its key features.  \n   - Highlight the problem it solves and its target audience or use cases.  \n\n   ### 2. System Architecture  \n   - Describe the high-level architecture, focusing on main components and their interactions.  \n   - Include a diagram illustrating the architecture.  \n\n   ### 3. Components  \n   - For each major component, describe:  \n     - Purpose and core functionality  \n     - Technologies used  \n     - Interactions with other components  \n   - Use diagrams, tables, or charts where helpful.  \n\n   ### 4. Installation & Deployment  \n   - Provide step-by-step setup instructions, including:  \n     - Prerequisites (e.g., software, hardware)  \n     - Environment setup  \n     - Deployment commands and scripts  \n\n   ### 5. Configuration  \n   - Detail how to configure the system, including:  \n     - Environment variables  \n     - Configuration files (with examples)  \n     - Key settings and their impact  \n\n   ### 6. Usage  \n   - Explain how to use the system, including:  \n     - API endpoints (if applicable)  \n     - Examples of system usage  \n     - Instructions for performing common operations  \n\n   ### 7. API Reference  \n   - Provide a detailed reference for all available APIs, including:  \n     - Endpoint descriptions  \n     - Input/output parameters  \n     - Example requests and responses  \n\n   ### 8. Security Considerations  \n   - Outline security best practices, such as:  \n     - Authentication and authorization methods  \n     - Data encryption practices  \n     - Known vulnerabilities and mitigation strategies  \n\n   ### 9. Monitoring & Logging  \n   - Explain how to monitor system performance and access logs, covering:  \n     - Key metrics  \n     - Tools or dashboards  \n     - Log management and analysis  \n\n   ### 10. Troubleshooting  \n   - Offer guidance on resolving common issues, including:  \n     - Error messages and their meanings  \n     - Debugging steps  \n     - Links to additional resources or forums  \n\n   ### 11. Development Guide  \n   - Provide information for developers working on the project, such as:  \n     - Codebase organization and key files  \n     - Development environment setup  \n     - Testing and debugging procedures  \n\n   ### 12. Maintenance & Operations  \n   - Describe ongoing maintenance and operational tasks, including:  \n     - Scheduled backups  \n     - Regular updates and patches  \n     - Monitoring long-term system health  \n\n4. **Examples & Visuals:**  \n   - Include **examples** of common scenarios, API calls, and configurations.  \n   - Add **visual aids** (e.g., diagrams, charts) to improve understanding.  \n   - Ensure examples are realistic and easy to follow.  \n\n5. **Clarity & Accessibility:**  \n   - Use simple, precise language and avoid unnecessary jargon.  \n   - Make the documentation navigable with proper sectioning and linking.  \n\nThe resulting document should be a polished, professional README or technical guide that effectively supports users in deploying, maintaining, and extending the project.\n\"\"\"\n\n# Calculate the length and word count of the instruction prompt\nINSTRUCTION_CHARS = len(INSTRUCTION_PROMPT)\nINSTRUCTION_WORDS = len(INSTRUCTION_PROMPT.split())\n\n\ndef process_file(file_path):\n    \"\"\"\n    Processes a file by reading its content and splitting it into chunks based on character and word limits.\n\n    Args:\n        file_path (str): The path to the file to be processed.\n\n    Returns:\n        list: A list of tuples containing metadata and chunk content.\n    \"\"\"\n    logging.info(f\"Processing file: {file_path}\")\n    chunks = []\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.readlines()\n\n        current_chunk = []\n        current_chars = 0\n        current_words = 0\n\n        for line in content:\n            line_words = line.split()\n            line_chars = len(line)\n\n            if (\n                current_chars + line_chars > DEFAULT_MAX_CHARS - INSTRUCTION_CHARS\n                or current_words + len(line_words)\n                > DEFAULT_MAX_WORDS - INSTRUCTION_WORDS\n            ):\n                chunk = \"\".join(current_chunk)\n                metadata = f\"### File: {file_path}\\n### Chunk: {len(chunks) + 1}\\n\"\n                chunks.append((metadata, chunk))\n                current_chunk = []\n                current_chars = 0\n                current_words = 0\n\n            current_chunk.append(line)\n            current_chars += line_chars\n            current_words += len(line_words)\n\n        if current_chunk:\n            chunk = \"\".join(current_chunk)\n            metadata = f\"### File: {file_path}\\n### Chunk: {len(chunks) + 1}\\n\"\n            chunks.append((metadata, chunk))\n\n        logging.info(f\"File processed: {file_path}, Total chunks: {len(chunks)}\")\n\n    except Exception as e:\n        logging.error(f\"Error reading file {file_path}: {e}\")\n\n    return chunks\n\n\ndef combine_chunks(all_chunks):\n    \"\"\"\n    Combines chunks into larger chunks based on character and word limits.\n\n    Args:\n        all_chunks (list): A list of tuples containing metadata and chunk content.\n\n    Returns:\n        list: A list of combined chunks.\n    \"\"\"\n    logging.info(\"Combining chunks...\")\n    combined_chunks = []\n    current_metadata = []\n    current_chunk = []\n    current_chars = 0\n    current_words = 0\n\n    for metadata, chunk in all_chunks:\n        chunk_chars = len(chunk)\n        chunk_words = len(chunk.split())\n\n        if current_chars + chunk_chars > (\n            DEFAULT_MAX_CHARS - INSTRUCTION_CHARS\n        ) or current_words + chunk_words > (DEFAULT_MAX_WORDS - INSTRUCTION_WORDS):\n            combined_chunks.append((current_metadata, current_chunk))\n            current_metadata = []\n            current_chunk = []\n            current_chars = 0\n            current_words = 0\n\n        current_metadata.append(metadata)\n        current_chunk.append(chunk)\n        current_chars += chunk_chars\n        current_words += chunk_words\n\n    if current_chunk:\n        combined_chunks.append((current_metadata, current_chunk))\n\n    logging.info(\n        f\"Chunks combined into {len(combined_chunks)} chunks to reduce the number of requests.\"\n    )\n    return combined_chunks\n\n\ndef combine_results(results):\n    \"\"\"\n    Combines results into larger chunks based on character and word limits.\n\n    Args:\n        results (list): A list of result strings.\n\n    Returns:\n        list: A list of combined results.\n    \"\"\"\n    logging.info(\"Combining results...\")\n    MAX_CHARS = DEFAULT_MAX_CHARS - INSTRUCTION_CHARS\n    MAX_WORDS = DEFAULT_MAX_WORDS - INSTRUCTION_WORDS\n\n    combined = []\n    str_combined = \"\"\n    current_chars = 0\n    current_words = 0\n\n    for result in results:\n        result_chars = len(result)\n        result_words = len(result.split())\n\n        if (\n            current_chars + result_chars <= MAX_CHARS\n            and current_words + result_words <= MAX_WORDS\n        ):\n            str_combined += result\n            current_chars += result_chars\n            current_words += result_words\n        else:\n            combined.append(\n                (\n                    \"Merge these documents into a consolidated single document.\",\n                    str_combined,\n                )\n            )\n            str_combined = result\n            current_chars = result_chars\n            current_words = result_words\n\n    if str_combined:\n        combined.append(\n            (\"Merge these documents into a consolidated single document.\", str_combined)\n        )\n\n    logging.info(f\"Results combined into {len(combined)} final combined results.\")\n    return combined\n\n\ndef read_files_with_chunking(\n    directory, file_extensions=None, exclude_folders=None, exclude_files=None\n):\n    \"\"\"\n    Reads all files in a directory, applies exclusions, and chunks content.\n\n    Args:\n        directory (str): The directory to read files from.\n        file_extensions (list, optional): List of file extensions to include. Defaults to None.\n        exclude_folders (list, optional): List of folders to exclude. Defaults to None.\n        exclude_files (list, optional): List of specific files to exclude. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing a list of chunks and the processing type.\n    \"\"\"\n    logging.info(f\"Starting to read files from directory: {directory}\")\n    exclude_folders = set(exclude_folders or [])\n    exclude_files = set(exclude_files or [])\n    file_extensions = file_extensions or []\n\n    file_paths = [\n        os.path.join(root, file)\n        for root, dirs, files in os.walk(directory)\n        if not any(folder in root for folder in exclude_folders)\n        for file in files\n        if (not file_extensions or any(file.endswith(ext) for ext in file_extensions))\n        and file not in exclude_files\n    ]\n\n    logging.info(f\"Total files found: {len(file_paths)}\")\n    all_chunks = []\n    total_chars = 0\n    total_words = 0\n\n    for file_path in file_paths:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n            total_chars += len(content)\n            total_words += len(content.split())\n\n    PROCESSING_TYPE = (\n        \"sequential\"\n        if total_chars <= (DEFAULT_MAX_CHARS - INSTRUCTION_CHARS) * CHUNK_LIMIT\n        and total_words <= (DEFAULT_MAX_WORDS - INSTRUCTION_WORDS) * CHUNK_LIMIT\n        else \"parallel\"\n    )\n\n    logging.info(f\"Processing mode determined: {PROCESSING_TYPE}\")\n    if PROCESSING_TYPE == \"sequential\":\n        logging.info(\"Processing files sequentially...\")\n        for file_path in file_paths:\n            all_chunks.extend(process_file(file_path))\n    else:\n        logging.info(\"Processing files in parallel...\")\n        with ThreadPoolExecutor() as executor:\n            results = executor.map(process_file, file_paths)\n            for chunks in results:\n                all_chunks.extend(chunks)\n\n    logging.info(f\"Total chunks created: {len(all_chunks)}\")\n\n    # Combine chunks to reduce the number of requests\n    combined_chunks = combine_chunks(all_chunks)\n    with open(\"logs/combined_chunks.json\", \"w\") as f:\n        json.dump(combined_chunks, f)\n    return combined_chunks, PROCESSING_TYPE\n\n\ndef ask_claude_batch(\n    context_chunks,\n    PROCESSING_TYPE,\n    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n):\n    \"\"\"\n    Sends chunks to the Claude model for processing and consolidates the responses.\n\n    Args:\n        context_chunks (list): List of context chunks to be processed.\n        PROCESSING_TYPE (str): The processing type (sequential or parallel).\n        model_id (str, optional): The model ID to use. Defaults to \"anthropic.claude-3-5-sonnet-20240620-v1:0\".\n\n    Returns:\n        str: The final consolidated response.\n    \"\"\"\n    client = boto3.client(\n        service_name=\"bedrock-runtime\",\n        region_name=\"us-east-1\",\n        config=Config(read_timeout=100000),\n    )\n\n    def invoke_model(metadata, chunk):\n        \"\"\"\n        Invokes the Claude model for a single chunk.\n\n        Args:\n            metadata (str): Metadata for the chunk.\n            chunk (str): The chunk content.\n\n        Returns:\n            str: The response text from the model.\n        \"\"\"\n        prompt = f\"\"\"\n        {INSTRUCTION_PROMPT}\n\n        ### Context:\n        {metadata}\n        {chunk}\n        \"\"\"\n        request_payload = {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 8192,\n            \"temperature\": 0.7,\n            \"messages\": [\n                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n            ],\n        }\n        response = client.invoke_model(\n            modelId=model_id, body=json.dumps(request_payload)\n        )\n        return json.loads(response[\"body\"].read())[\"content\"][0][\"text\"]\n\n    # Configuration for batching\n    BATCH_SIZE = 50  # Number of requests per batch\n    TIME_WINDOW = 60  # Time window in seconds for the batch\n\n    # Function to invoke the model with retry logic\n    def invoke_model_with_retry(metadata, chunk, retries=10, backoff_factor=2):\n        \"\"\"\n        Invokes the model with retry logic in case of failures.\n\n        Args:\n            metadata (str): Metadata for the chunk.\n            chunk (str): The chunk content.\n            retries (int, optional): Number of retry attempts. Defaults to 10.\n            backoff_factor (int, optional): Backoff factor for retry delay. Defaults to 2.\n\n        Returns:\n            str: The response text from the model.\n        \"\"\"\n        delay = 30  # Initial delay in seconds\n        for attempt in range(retries):\n            try:\n                return invoke_model(metadata, chunk)\n            except Exception:\n                if attempt < retries - 1:\n                    wait_time = delay * (backoff_factor**attempt) + random.uniform(\n                        5, 10\n                    )\n                    logging.info(\n                        f\"ThrottlingException: Retrying in {wait_time:.2f} seconds...\"\n                    )\n                    time.sleep(wait_time)\n                else:\n                    logging.info(\"Maximum retry attempts reached.\")\n                    raise\n\n    # Function to process chunks in batches\n    def process_in_batches(context_chunks, batch_size, time_window):\n        \"\"\"\n        Processes chunks in batches.\n\n        Args:\n            context_chunks (list): List of context chunks to be processed.\n            batch_size (int): Number of requests per batch.\n            time_window (int): Time window in seconds for the batch.\n\n        Returns:\n            list: List of results from the model.\n        \"\"\"\n        results = []\n        batch_start = 0\n        while batch_start < len(context_chunks):\n            batch_end = min(batch_start + batch_size, len(context_chunks))\n            batch = context_chunks[batch_start:batch_end]\n\n            logging.info(\n                f\"Processing batch {batch_start // batch_size + 1} (chunks {batch_start}-{batch_end - 1})...\"\n            )\n            with ThreadPoolExecutor() as executor:\n                batch_results = list(\n                    executor.map(lambda x: invoke_model_with_retry(*x), batch)\n                )\n            results.extend(batch_results)\n\n            # Wait for the remainder of the time window if needed\n            if batch_end < len(context_chunks):  # Skip delay for the last batch\n                time.sleep(time_window)\n\n            batch_start = batch_end\n\n        return results\n\n    # Main logic for processing chunks\n    if PROCESSING_TYPE == \"parallel\":\n        logging.info(\"Processing chunks with Claude in parallel using batching...\")\n\n        # Process chunks in batches\n        results = process_in_batches(context_chunks, BATCH_SIZE, TIME_WINDOW)\n\n        # Combine chunks to reduce the number of requests\n        combined_results = combine_results(results)\n\n        # Process results in batches\n        results = process_in_batches(combined_results, BATCH_SIZE, TIME_WINDOW)\n\n        # Consolidate results into a single document\n        consolidated_documents = \"\\n\\n\".join(results)\n        final_results = invoke_model(\n            \"Merge these documents into a consolidated single document.\",\n            consolidated_documents,\n        )\n\n        return final_results\n\n    else:\n        # Process chunks sequentially and consolidate results\n        logging.info(\"Processing chunks with Claude sequentially...\")\n        consolidated_context = \"\"\n        final_response = \"\"\n        for metadata, chunk in context_chunks:\n            response_text = invoke_model(\n                \"### Previous Context:\\n\"\n                + consolidated_context\n                + \"\\n\\n### Current Context:\\n\"\n                + str(metadata),\n                str(chunk),\n            )\n            consolidated_context = response_text\n\n        final_response = consolidated_context\n        return final_response\n\n\ndef main(\n    directory,\n    file_extensions=None,\n    exclude_folders=None,\n    exclude_files=None,\n    output_file=\"README.md\",\n    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n):\n    \"\"\"\n    Main function to process all files and save the output.\n\n    Args:\n        directory (str): The directory to read files from.\n        file_extensions (list, optional): List of file extensions to include. Defaults to None.\n        exclude_folders (list, optional): List of folders to exclude. Defaults to None.\n        output_file (str, optional): The output file name. Defaults to \"README.md\".\n    \"\"\"\n    logging.info(\"Starting main process...\")\n    chunks, PROCESSING_TYPE = read_files_with_chunking(\n        directory, file_extensions, exclude_folders, exclude_files\n    )\n\n    logging.info(\"Sending chunks for processing...\")\n    responses = ask_claude_batch(chunks, PROCESSING_TYPE, model_id)\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(responses)\n\n    logging.info(f\"Processing complete. Output saved to: {output_file}\")\n\n\n# Entry point for the script\nif __name__ == \"__main__\":\n    DIRECTORY = \"/Users/mdf/Code/SwiftDocsAI\"\n    FILE_EXTENSIONS = [\n        \".ts\",\n        \".js\",\n        \".py\",\n        \".ksh\",\n        \".yaml\",\n        \".md\",\n        \"Dockerfile\",\n        \".yml\",\n        \".txt\",\n        \".env\",\n    ]  # Use empty list or None to process all file types.\n    EXCLUDE_FOLDERS = [\".git\", \".venv\", \"node_modules\", \"logs\"]\n    EXCLUDE_FILES = [\n        \"README.md\",\n        \"LICENSE\",\n        \"CONTRIBUTING.md\",\n        \"CODE_OF_CONDUCT.md\",\n        \".DS_Store\",\n    ]\n    OUTPUT_FILE = \"README.md\"\n    MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n\n    try:\n        main(\n            DIRECTORY,\n            FILE_EXTENSIONS,\n            EXCLUDE_FOLDERS,\n            EXCLUDE_FILES,\n            OUTPUT_FILE,\n            MODEL_ID,\n        )\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n        raise e\n"]]]